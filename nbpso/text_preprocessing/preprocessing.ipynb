{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan):\n",
    "    # Membaca file CSV dengan memisahkan kolom berdasarkan delimiter ';'\n",
    "    df = pd.read_csv(file_input, delimiter=';')\n",
    "\n",
    "    # Mengecek nama kolom yang ada dalam DataFrame\n",
    "    print(\"Nama Kolom dalam DataFrame:\", df.columns)\n",
    "\n",
    "    try:\n",
    "        # Menghapus kolom yang tidak digunakan\n",
    "        df = df[kolom_yang_digunakan]\n",
    "\n",
    "        # Menyimpan DataFrame yang sudah dimodifikasi ke file CSV baru\n",
    "        df.to_csv(file_output, index=False)\n",
    "\n",
    "        print(f\"File {file_output} telah berhasil dibuat dengan kolom yang diinginkan.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e}. Pastikan nama kolom yang digunakan sesuai dengan nama kolom dalam file CSV.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ganti nama file_input.csv dengan nama file CSV yang ingin Anda proses\n",
    "    file_input = 'dataset.csv'\n",
    "\n",
    "    # Ganti nama file_output.csv dengan nama file CSV yang ingin Anda hasilkan\n",
    "    file_output = 'datasetfix.csv'\n",
    "\n",
    "    # Ganti kolom_yang_digunakan dengan daftar kolom yang ingin Anda pertahankan\n",
    "    kolom_yang_digunakan = ['Sentiment', 'text_tweet']\n",
    "\n",
    "    # Memanggil fungsi untuk menghapus kolom yang tidak digunakan\n",
    "    hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>Undang @N_ShaniJKT48 ke hitamputih, pemenang S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>Selamat berbuka puasa Semoga amal ibadah hari ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>Ada nih di trans7 hitam putih, dia dpt penghar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>selamat ya mas @adietaufan masuk hitamputih</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>Asiknya nonton Hitam Putih Trans7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Sentiment                                              tweet\n",
       "0  positive  Undang @N_ShaniJKT48 ke hitamputih, pemenang S...\n",
       "1  positive  Selamat berbuka puasa Semoga amal ibadah hari ...\n",
       "2  positive  Ada nih di trans7 hitam putih, dia dpt penghar...\n",
       "3  positive        selamat ya mas @adietaufan masuk hitamputih\n",
       "4  positive                  Asiknya nonton Hitam Putih Trans7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "TWEET_DATA = pd.read_csv(\"datasetfix.csv\", encoding = \"ISO-8859-1\")\n",
    "TWEET_DATA['tweet'].str.encode('ascii', 'ignore')\n",
    "TWEET_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0    undang @n_shanijkt48 ke hitamputih, pemenang s...\n",
      "1    selamat berbuka puasa semoga amal ibadah hari ...\n",
      "2    ada nih di trans7 hitam putih, dia dpt penghar...\n",
      "3          selamat ya mas @adietaufan masuk hitamputih\n",
      "4                    asiknya nonton hitam putih trans7\n",
      "Name: tweet, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(TWEET_DATA['tweet'].head(5))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [undang, shanijkt, ke, hitamputih, pemenang, s...\n",
      "1    [selamat, berbuka, puasa, semoga, amal, ibadah...\n",
      "2    [ada, nih, di, trans, hitam, putih, dia, dpt, ...\n",
      "3                [selamat, ya, mas, masuk, hitamputih]\n",
      "4               [asiknya, nonton, hitam, putih, trans]\n",
      "Name: tweet_tokens, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "TWEET_DATA['tweet'] = TWEET_DATA['tweet'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "TWEET_DATA['tweet_tokens'] = TWEET_DATA['tweet'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(TWEET_DATA['tweet_tokens'].head())\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0    [(undang, 2), (shanijkt, 1), (ke, 1), (hitampu...\n",
      "1    [(selamat, 1), (berbuka, 1), (puasa, 1), (semo...\n",
      "2    [(di, 2), (ada, 1), (nih, 1), (trans, 1), (hit...\n",
      "3    [(selamat, 1), (ya, 1), (mas, 1), (masuk, 1), ...\n",
      "4    [(asiknya, 1), (nonton, 1), (hitam, 1), (putih...\n",
      "Name: tweet_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "TWEET_DATA['tweet_tokens_fdist'] = TWEET_DATA['tweet_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(TWEET_DATA['tweet_tokens_fdist'].head().apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n",
      "0    [undang, shanijkt, hitamputih, pemenang, ssk, ...\n",
      "1    [selamat, berbuka, puasa, semoga, amal, ibadah...\n",
      "2         [trans, hitam, putih, penghargaan, norwegia]\n",
      "3                                [selamat, hitamputih]\n",
      "4               [asiknya, nonton, hitam, putih, trans]\n",
      "Name: tweet_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "print(len(list_stopwords))\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "len(list_stopwords)\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"stopword.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "len(list_stopwords)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "TWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(TWEET_DATA['tweet_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "------------------------\n",
      " : \n",
      "{'': ''}\n",
      "------------------------\n",
      "0      [undang, shanijkt, hitamputih, pemenang, ssk, ...\n",
      "1      [selamat, berbuka, puasa, semoga, amal, ibadah...\n",
      "2           [trans, hitam, putih, penghargaan, norwegia]\n",
      "3                                  [selamat, hitamputih]\n",
      "4                 [asiknya, nonton, hitam, putih, trans]\n",
      "                             ...                        \n",
      "395            [banget, kesel, debat, pake, emosi, gini]\n",
      "396            [miskin, miskin, sekolah, pungutan, liar]\n",
      "397           [emosi, cepat, tua, nonton, emosi, bicara]\n",
      "398    [penampilan, kyk, preman, taunya, bkin, kisruh...\n",
      "399                  [berbelitbelit, muter, buang, mutu]\n",
      "Name: tweet_tokens_stemmed, Length: 400, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Load model bahasa Indonesia dari spaCy\n",
    "nlp_id = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# Membuat objek Stemmer dari Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Normalisasi kata dengan spaCy Bahasa Indonesia\n",
    "def normalized_term_id(document):\n",
    "    # Tokenisasi dokumen menggunakan spaCy\n",
    "    doc = nlp_id(document)\n",
    "    \n",
    "    # Normalisasi kata dalam dokumen\n",
    "    normalized_document = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "    \n",
    "    return normalized_document\n",
    "\n",
    "# Normalisasi kata dengan Sastrawi\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "# Membuat kamus untuk normalisasi kata\n",
    "term_dict = {}\n",
    "\n",
    "# Normalisasi kata dengan spaCy dan Sastrawi\n",
    "for document in TWEET_DATA['tweet_tokens_WSW']:\n",
    "    normalized_document_id = normalized_term_id(' '.join(document))\n",
    "    normalized_document_stemmed = [stemmed_wrapper(term) for term in normalized_document_id]\n",
    "    \n",
    "    for term in normalized_document_stemmed:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "# Menampilkan jumlah kata yang sudah dinormalisasi dan di-stem\n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Melakukan stemming pada kamus term_dict\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term, \":\", term_dict[term])\n",
    "\n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Menerapkan term yang sudah di-stem pada dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict.get(term, term) for term in document]\n",
    "\n",
    "\n",
    "TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_tokens_WSW'].apply(get_stemmed_term)\n",
    "print(TWEET_DATA['tweet_tokens_stemmed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_DATA.to_csv(\"Text_Preprocessing.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentiment                               tweet_tokens_stemmed\n",
      "0    positive  ['undang', 'shanijkt', 'hitamputih', 'pemenang...\n",
      "1    positive  ['selamat', 'berbuka', 'puasa', 'semoga', 'ama...\n",
      "2    positive  ['trans', 'hitam', 'putih', 'penghargaan', 'no...\n",
      "3    positive                          ['selamat', 'hitamputih']\n",
      "4    positive   ['asiknya', 'nonton', 'hitam', 'putih', 'trans']\n",
      "..        ...                                                ...\n",
      "395  negative  ['banget', 'kesel', 'debat', 'pake', 'emosi', ...\n",
      "396  negative  ['miskin', 'miskin', 'sekolah', 'pungutan', 'l...\n",
      "397  negative  ['emosi', 'cepat', 'tua', 'nonton', 'emosi', '...\n",
      "398  negative  ['penampilan', 'kyk', 'preman', 'taunya', 'bki...\n",
      "399  negative        ['berbelitbelit', 'muter', 'buang', 'mutu']\n",
      "\n",
      "[400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Membaca file CSV dan mengabaikan kolom indeks 'Unnamed: 0'\n",
    "text_preprocessing_data = pd.read_csv('text_preprocessing.csv', index_col=0)\n",
    "\n",
    "# Memilih hanya kolom 'sentiment' dan 'tweet_tokens_stemmed'\n",
    "selected_columns = text_preprocessing_data[['Sentiment', 'tweet_tokens_stemmed']]\n",
    "\n",
    "# Menyimpan DataFrame yang telah dipilih ke dalam file CSV baru\n",
    "selected_columns.to_csv('final_dataset.csv', index=False)\n",
    "\n",
    "# Menampilkan DataFrame yang telah dipilih\n",
    "print(selected_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

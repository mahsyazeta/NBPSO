{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan):\n",
    "    df = pd.read_csv(file_input, delimiter=';')\n",
    "    df = df[kolom_yang_digunakan]\n",
    "    df.to_csv(file_output, index=False)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "    return text\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_input = 'dataset.csv'\n",
    "    file_output = 'datasetfix.csv'\n",
    "    kolom_yang_digunakan = ['Sentiment', 'Text Tweet']\n",
    "\n",
    "    hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan)\n",
    "\n",
    "    TWEET_DATA = pd.read_csv(file_output, encoding=\"ISO-8859-1\")\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(preprocess_text)\n",
    "    TWEET_DATA['tweet_tokens'] = TWEET_DATA['Text Tweet'].apply(word_tokenize)\n",
    "\n",
    "    list_stopwords = stopwords.words('indonesian')\n",
    "    list_stopwords.extend([\n",
    "        \"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "        'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "        'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt', '&amp', 'yah'\n",
    "    ])\n",
    "    txt_stopword = pd.read_csv(\"stopword.txt\", names=[\"stopwords\"], header=None)\n",
    "    list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "    list_stopwords = set(list_stopwords)\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal)\n",
    "\n",
    "    nlp_id = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_tokens_WSW'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
    "    TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_tokens_stemmed'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "    selected_columns = TWEET_DATA[['Sentiment', 'tweet_tokens_stemmed']]\n",
    "    selected_columns.to_csv('final_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentiment                               tweet_tokens_stemmed\n",
      "0    positive  undang shanijkt hitamputih menang ssk jkt mjkt...\n",
      "1    positive   selamat buka puasa moga amal ibadah terima allah\n",
      "2    positive                   trans hitam putih harga norwegia\n",
      "3    positive                                 selamat hitamputih\n",
      "4    positive                   asiknya nonton hitam putih trans\n",
      "..        ...                                                ...\n",
      "395  negative                  banget kesel debat pake emosi gin\n",
      "396  negative                  miskin miskin sekolah pungut liar\n",
      "397  negative                emosi cepat tua nonton emosi bicara\n",
      "398  negative             tampil kyk preman tau bkin kisruh usak\n",
      "399  negative                     berbelitbelit muter buang mutu\n",
      "\n",
      "[400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(selected_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

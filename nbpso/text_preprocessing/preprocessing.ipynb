{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Kolom dalam DataFrame: Index(['Id', 'Sentiment', 'Acara TV', 'Jumlah Retweet', 'Text Tweet'], dtype='object')\n",
      "File datasetfix.csv telah berhasil dibuat dengan kolom yang diinginkan.\n",
      "    Sentiment                               tweet_tokens_stemmed\n",
      "0    positive  ['undang', 'shanijkt', 'hitamputih', 'pemenang...\n",
      "1    positive  ['selamat', 'berbuka', 'puasa', 'semoga', 'ama...\n",
      "2    positive  ['trans', 'hitam', 'putih', 'penghargaan', 'no...\n",
      "3    positive                          ['selamat', 'hitamputih']\n",
      "4    positive   ['asiknya', 'nonton', 'hitam', 'putih', 'trans']\n",
      "..        ...                                                ...\n",
      "395  negative  ['banget', 'kesel', 'debat', 'pake', 'emosi', ...\n",
      "396  negative  ['miskin', 'miskin', 'sekolah', 'pungutan', 'l...\n",
      "397  negative  ['emosi', 'cepat', 'tua', 'nonton', 'emosi', '...\n",
      "398  negative  ['penampilan', 'kyk', 'preman', 'taunya', 'bki...\n",
      "399  negative        ['berbelitbelit', 'muter', 'buang', 'mutu']\n",
      "\n",
      "[400 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "def hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan):\n",
    "   \n",
    "    df = pd.read_csv(file_input, delimiter=';')\n",
    "    print(\"Nama Kolom dalam DataFrame:\", df.columns)\n",
    "\n",
    "    try:\n",
    "        df = df[kolom_yang_digunakan]\n",
    "        df.to_csv(file_output, index=False)\n",
    "        print(f\"File {file_output} telah berhasil dibuat dengan kolom yang diinginkan.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e}. Pastikan nama kolom yang digunakan sesuai dengan nama kolom dalam file CSV.\")\n",
    "\n",
    "def case_folding(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    text = text.replace('\\\\t', \" \").replace('\\\\n', \" \").replace('\\\\u', \" \").replace('\\\\', \"\")\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\", \" \", text).split())\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "\n",
    "def remove_number(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+', ' ', text)\n",
    "\n",
    "def remove_single_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "def normalized_term_id(document):\n",
    "    doc = nlp_id(document)\n",
    "    return [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict.get(term, term) for term in document]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_input = 'dataset.csv'\n",
    "    file_output = 'datasetfix.csv'\n",
    "    kolom_yang_digunakan = ['Sentiment', 'Text Tweet']\n",
    "\n",
    "    hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan)\n",
    "\n",
    "    TWEET_DATA = pd.read_csv(file_output, encoding=\"ISO-8859-1\")\n",
    "\n",
    "    if 'Text Tweet' not in TWEET_DATA.columns:\n",
    "        raise KeyError(\"'Text Tweet' column not found in the input file. Please check the column name.\")\n",
    "\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(case_folding)\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_tweet_special)\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_number)\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_punctuation)\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_whitespace_LT)\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_whitespace_multiple)\n",
    "    TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_single_char)\n",
    "    TWEET_DATA['tweet_tokens'] = TWEET_DATA['Text Tweet'].apply(word_tokenize_wrapper)\n",
    "    TWEET_DATA['tweet_tokens_fdist'] = TWEET_DATA['tweet_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "    list_stopwords = stopwords.words('indonesian')\n",
    "    list_stopwords.extend([\n",
    "        \"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', 'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "        'gak', 'ga', 'krn', 'nya', 'nih', 'sih', 'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "        'jd', 'jgn', 'sdh', 'aja', 'n', 't', 'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt', '&amp', 'yah'\n",
    "    ])\n",
    "    txt_stopword = pd.read_csv(\"stopword.txt\", names=[\"stopwords\"], header=None)\n",
    "    list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "    list_stopwords = set(list_stopwords)\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal)\n",
    "\n",
    "    nlp_id = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    factory = StemmerFactory()\n",
    "    stemmer = factory.create_stemmer()\n",
    "    term_dict = {}\n",
    "\n",
    "    for document in TWEET_DATA['tweet_tokens_WSW']:\n",
    "        normalized_document_id = normalized_term_id(' '.join(document))\n",
    "        normalized_document_stemmed = [stemmed_wrapper(term) for term in normalized_document_id]\n",
    "        for term in normalized_document_stemmed:\n",
    "            if term not in term_dict:\n",
    "                term_dict[term] = ' '\n",
    "\n",
    "    for term in term_dict:\n",
    "        term_dict[term] = stemmed_wrapper(term)\n",
    "\n",
    "    TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_tokens_WSW'].apply(get_stemmed_term)\n",
    "    TWEET_DATA.to_csv(\"Text_Preprocessing2.csv\")\n",
    "\n",
    "    text_preprocessing_data = pd.read_csv('Text_Preprocessing2.csv', index_col=0)\n",
    "    selected_columns = text_preprocessing_data[['Sentiment', 'tweet_tokens_stemmed']]\n",
    "    selected_columns.to_csv('final_dataset.csv', index=False)\n",
    "\n",
    "    print(selected_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nama Kolom dalam DataFrame: Index(['Id,Sentiment,Pasangan Calon,Text Tweet'], dtype='object')\n",
      "Error: \"None of [Index(['Sentiment', 'Text Tweet'], dtype='object')] are in the [columns]\". Pastikan nama kolom yang digunakan sesuai dengan nama kolom dalam file CSV.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan):\n",
    "    # Membaca file CSV dengan memisahkan kolom berdasarkan delimiter ';'\n",
    "    df = pd.read_csv(file_input, delimiter=';')\n",
    "\n",
    "    # Mengecek nama kolom yang ada dalam DataFrame\n",
    "    print(\"Nama Kolom dalam DataFrame:\", df.columns)\n",
    "\n",
    "    try:\n",
    "        # Menghapus kolom yang tidak digunakan\n",
    "        df = df[kolom_yang_digunakan]\n",
    "\n",
    "        # Menyimpan DataFrame yang sudah dimodifikasi ke file CSV baru\n",
    "        df.to_csv(file_output, index=False)\n",
    "\n",
    "        print(f\"File {file_output} telah berhasil dibuat dengan kolom yang diinginkan.\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: {e}. Pastikan nama kolom yang digunakan sesuai dengan nama kolom dalam file CSV.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ganti nama file_input.csv dengan nama file CSV yang ingin Anda proses\n",
    "    file_input = 'dataset2.csv'\n",
    "\n",
    "    # Ganti nama file_output.csv dengan nama file CSV yang ingin Anda hasilkan\n",
    "    file_output = 'datasetfix2.csv'\n",
    "\n",
    "    # Ganti kolom_yang_digunakan dengan daftar kolom yang ingin Anda pertahankan\n",
    "    kolom_yang_digunakan = ['Sentiment', 'Text Tweet']\n",
    "\n",
    "    # Memanggil fungsi untuk menghapus kolom yang tidak digunakan\n",
    "    hapus_kolom_tidak_digunakan(file_input, file_output, kolom_yang_digunakan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ï»¿Id</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Pasangan Calon</th>\n",
       "      <th>Text Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Banyak akun kloning seolah2 pendukung #agussil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ï»¿Id Sentiment Pasangan Calon  \\\n",
       "0      1  negative     Agus-Sylvi   \n",
       "1      2  negative     Agus-Sylvi   \n",
       "2      3  negative     Agus-Sylvi   \n",
       "3      4  negative     Agus-Sylvi   \n",
       "4      5  negative     Agus-Sylvi   \n",
       "\n",
       "                                          Text Tweet  \n",
       "0  Banyak akun kloning seolah2 pendukung #agussil...  \n",
       "1  #agussilvy bicara apa kasihan yaa...lap itu ai...  \n",
       "2  Kalau aku sih gak nunggu hasil akhir QC tp lag...  \n",
       "3  Kasian oh kasian dengan peluru 1milyar untuk t...  \n",
       "4  Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "TWEET_DATA = pd.read_csv(\"dataset2.csv\", encoding = \"ISO-8859-1\")\n",
    "TWEET_DATA['Text Tweet'].str.encode('ascii', 'ignore')\n",
    "TWEET_DATA.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case Folding Result : \n",
      "\n",
      "0    banyak akun kloning seolah2 pendukung #agussil...\n",
      "1    #agussilvy bicara apa kasihan yaa...lap itu ai...\n",
      "2    kalau aku sih gak nunggu hasil akhir qc tp lag...\n",
      "3    kasian oh kasian dengan peluru 1milyar untuk t...\n",
      "4    maaf ya pendukung #agussilvy..hayo dukung #ani...\n",
      "Name: Text Tweet, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------ Case Folding --------\n",
    "# gunakan fungsi Series.str.lower() pada Pandas\n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].str.lower()\n",
    "\n",
    "\n",
    "print('Case Folding Result : \\n')\n",
    "print(TWEET_DATA['Text Tweet'].head(5))\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing Result : \n",
      "\n",
      "0    [banyak, akun, kloning, seolah, pendukung, mul...\n",
      "1    [bicara, apa, kasihan, yaalap, itu, air, matan...\n",
      "2    [kalau, aku, sih, gak, nunggu, hasil, akhir, q...\n",
      "3    [kasian, oh, kasian, dengan, peluru, milyar, u...\n",
      "4       [maaf, ya, pendukung, hayo, dukung, diputaran]\n",
      "Name: tweet_tokens, dtype: object\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re #regex library\n",
    "\n",
    "# import word_tokenize & FreqDist from NLTK\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# ------ Tokenizing ---------\n",
    "\n",
    "def remove_tweet_special(text):\n",
    "    # remove tab, new line, ans back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # remove non ASCII (emoticon, chinese word, .etc)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # remove mention, link, hashtag\n",
    "    text = ' '.join(re.sub(\"([@#][A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # remove incomplete URL\n",
    "    return text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "                \n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_tweet_special)\n",
    "\n",
    "#remove number\n",
    "def remove_number(text):\n",
    "    return  re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_number)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "\n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_punctuation)\n",
    "\n",
    "#remove whitespace leading & trailing\n",
    "def remove_whitespace_LT(text):\n",
    "    return text.strip()\n",
    "\n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_whitespace_LT)\n",
    "\n",
    "#remove multiple whitespace into single whitespace\n",
    "def remove_whitespace_multiple(text):\n",
    "    return re.sub('\\s+',' ',text)\n",
    "\n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_whitespace_multiple)\n",
    "\n",
    "# remove single char\n",
    "def remove_singl_char(text):\n",
    "    return re.sub(r\"\\b[a-zA-Z]\\b\", \"\", text)\n",
    "\n",
    "TWEET_DATA['Text Tweet'] = TWEET_DATA['Text Tweet'].apply(remove_singl_char)\n",
    "\n",
    "# NLTK word rokenize \n",
    "def word_tokenize_wrapper(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "TWEET_DATA['tweet_tokens'] = TWEET_DATA['Text Tweet'].apply(word_tokenize_wrapper)\n",
    "\n",
    "print('Tokenizing Result : \\n') \n",
    "print(TWEET_DATA['tweet_tokens'].head())\n",
    "print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency Tokens : \n",
      "\n",
      "0    [(banyak, 1), (akun, 1), (kloning, 1), (seolah...\n",
      "1    [(bicara, 1), (apa, 1), (kasihan, 1), (yaalap,...\n",
      "2    [(nunggu, 2), (kalau, 1), (aku, 1), (sih, 1), ...\n",
      "3    [(kasian, 2), (oh, 1), (dengan, 1), (peluru, 1...\n",
      "4    [(maaf, 1), (ya, 1), (pendukung, 1), (hayo, 1)...\n",
      "Name: tweet_tokens_fdist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# NLTK calc frequency distribution\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)\n",
    "\n",
    "TWEET_DATA['tweet_tokens_fdist'] = TWEET_DATA['tweet_tokens'].apply(freqDist_wrapper)\n",
    "\n",
    "print('Frequency Tokens : \\n') \n",
    "print(TWEET_DATA['tweet_tokens_fdist'].head().apply(lambda x : x.most_common()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "758\n",
      "0    [akun, kloning, pendukung, menyerang, paslon, ...\n",
      "1    [bicara, kasihan, yaalap, air, matanya, wkwkwkwk]\n",
      "2    [nunggu, hasil, qc, nunggu, motif, cuitan, pas...\n",
      "3    [kasian, kasian, peluru, milyar, rw, mempan, m...\n",
      "4                 [maaf, pendukung, dukung, diputaran]\n",
      "Name: tweet_tokens_WSW, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ----------------------- get stopword from NLTK stopword -------------------------------\n",
    "# get stopword indonesia\n",
    "list_stopwords = stopwords.words('indonesian')\n",
    "print(len(list_stopwords))\n",
    "\n",
    "# ---------------------------- manualy add stopword  ------------------------------------\n",
    "# append additional stopword\n",
    "list_stopwords.extend([\"yg\", \"dg\", \"rt\", \"dgn\", \"ny\", \"d\", 'klo', \n",
    "                       'kalo', 'amp', 'biar', 'bikin', 'bilang', \n",
    "                       'gak', 'ga', 'krn', 'nya', 'nih', 'sih', \n",
    "                       'si', 'tau', 'tdk', 'tuh', 'utk', 'ya', \n",
    "                       'jd', 'jgn', 'sdh', 'aja', 'n', 't', \n",
    "                       'nyg', 'hehe', 'pen', 'u', 'nan', 'loh', 'rt',\n",
    "                       '&amp', 'yah'])\n",
    "len(list_stopwords)\n",
    "# ----------------------- add stopword from txt file ------------------------------------\n",
    "# read txt stopword using pandas\n",
    "txt_stopword = pd.read_csv(\"stopword.txt\", names= [\"stopwords\"], header = None)\n",
    "\n",
    "# convert stopword string to list & append additional stopword\n",
    "list_stopwords.extend(txt_stopword[\"stopwords\"][0].split(' '))\n",
    "len(list_stopwords)\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# convert list to dictionary\n",
    "list_stopwords = set(list_stopwords)\n",
    "\n",
    "\n",
    "#remove stopword pada list token\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]\n",
    "\n",
    "TWEET_DATA['tweet_tokens_WSW'] = TWEET_DATA['tweet_tokens'].apply(stopwords_removal) \n",
    "\n",
    "\n",
    "print(TWEET_DATA['tweet_tokens_WSW'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "------------------------\n",
      " : \n",
      "{'': ''}\n",
      "------------------------\n",
      "0      [akun, kloning, pendukung, menyerang, paslon, ...\n",
      "1      [bicara, kasihan, yaalap, air, matanya, wkwkwkwk]\n",
      "2      [nunggu, hasil, qc, nunggu, motif, cuitan, pas...\n",
      "3      [kasian, kasian, peluru, milyar, rw, mempan, m...\n",
      "4                   [maaf, pendukung, dukung, diputaran]\n",
      "                             ...                        \n",
      "895                           [bpk, rspun, selfie, hand]\n",
      "896    [merangkul, batas, usia, kelamin, hand, victor...\n",
      "897    [jagoanku, dibidang, digital, smiling, face, w...\n",
      "898                                                   []\n",
      "899    [sandiaga, bangun, rumah, simpel, dibanding, t...\n",
      "Name: tweet_tokens_stemmed, Length: 900, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Load model bahasa Indonesia dari spaCy\n",
    "nlp_id = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "# Membuat objek Stemmer dari Sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# Normalisasi kata dengan spaCy Bahasa Indonesia\n",
    "def normalized_term_id(document):\n",
    "    # Tokenisasi dokumen menggunakan spaCy\n",
    "    doc = nlp_id(document)\n",
    "    \n",
    "    # Normalisasi kata dalam dokumen\n",
    "    normalized_document = [token.lemma_ if token.lemma_ != '-PRON-' else token.text for token in doc]\n",
    "    \n",
    "    return normalized_document\n",
    "\n",
    "# Normalisasi kata dengan Sastrawi\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "# Membuat kamus untuk normalisasi kata\n",
    "term_dict = {}\n",
    "\n",
    "# Normalisasi kata dengan spaCy dan Sastrawi\n",
    "for document in TWEET_DATA['tweet_tokens_WSW']:\n",
    "    normalized_document_id = normalized_term_id(' '.join(document))\n",
    "    normalized_document_stemmed = [stemmed_wrapper(term) for term in normalized_document_id]\n",
    "    \n",
    "    for term in normalized_document_stemmed:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "\n",
    "# Menampilkan jumlah kata yang sudah dinormalisasi dan di-stem\n",
    "print(len(term_dict))\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Melakukan stemming pada kamus term_dict\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term, \":\", term_dict[term])\n",
    "\n",
    "print(term_dict)\n",
    "print(\"------------------------\")\n",
    "\n",
    "# Menerapkan term yang sudah di-stem pada dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict.get(term, term) for term in document]\n",
    "\n",
    "\n",
    "TWEET_DATA['tweet_tokens_stemmed'] = TWEET_DATA['tweet_tokens_WSW'].apply(get_stemmed_term)\n",
    "print(TWEET_DATA['tweet_tokens_stemmed'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TWEET_DATA.to_csv(\"Text_Preprocessing2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Sentiment                               tweet_tokens_stemmed\n",
      "0    negative  ['akun', 'kloning', 'pendukung', 'menyerang', ...\n",
      "1    negative  ['bicara', 'kasihan', 'yaalap', 'air', 'matany...\n",
      "2    negative  ['nunggu', 'hasil', 'qc', 'nunggu', 'motif', '...\n",
      "3    negative  ['kasian', 'kasian', 'peluru', 'milyar', 'rw',...\n",
      "4    negative       ['maaf', 'pendukung', 'dukung', 'diputaran']\n",
      "..        ...                                                ...\n",
      "895  positive                 ['bpk', 'rspun', 'selfie', 'hand']\n",
      "896  positive  ['merangkul', 'batas', 'usia', 'kelamin', 'han...\n",
      "897  positive  ['jagoanku', 'dibidang', 'digital', 'smiling',...\n",
      "898  positive                                                 []\n",
      "899  positive  ['sandiaga', 'bangun', 'rumah', 'simpel', 'dib...\n",
      "\n",
      "[900 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Membaca file CSV dan mengabaikan kolom indeks 'Unnamed: 0'\n",
    "text_preprocessing_data = pd.read_csv('text_preprocessing2.csv', index_col=0)\n",
    "\n",
    "# Memilih hanya kolom 'sentiment' dan 'tweet_tokens_stemmed'\n",
    "selected_columns = text_preprocessing_data[['Sentiment', 'tweet_tokens_stemmed']]\n",
    "\n",
    "# Menyimpan DataFrame yang telah dipilih ke dalam file CSV baru\n",
    "selected_columns.to_csv('final_dataset2.csv', index=False)\n",
    "\n",
    "# Menampilkan DataFrame yang telah dipilih\n",
    "print(selected_columns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>tweet_tokens_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>['undang', 'shanijkt', 'hitamputih', 'pemenang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>['selamat', 'berbuka', 'puasa', 'semoga', 'ama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>['trans', 'hitam', 'putih', 'penghargaan', 'no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>['selamat', 'hitamputih']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>['asiknya', 'nonton', 'hitam', 'putih', 'trans']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>negative</td>\n",
       "      <td>['banget', 'kesel', 'debat', 'pake', 'emosi', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>negative</td>\n",
       "      <td>['miskin', 'miskin', 'sekolah', 'pungutan', 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>negative</td>\n",
       "      <td>['emosi', 'cepat', 'tua', 'nonton', 'emosi', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>negative</td>\n",
       "      <td>['penampilan', 'kyk', 'preman', 'taunya', 'bki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>negative</td>\n",
       "      <td>['berbelitbelit', 'muter', 'buang', 'mutu']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentiment                               tweet_tokens_stemmed\n",
       "0    positive  ['undang', 'shanijkt', 'hitamputih', 'pemenang...\n",
       "1    positive  ['selamat', 'berbuka', 'puasa', 'semoga', 'ama...\n",
       "2    positive  ['trans', 'hitam', 'putih', 'penghargaan', 'no...\n",
       "3    positive                          ['selamat', 'hitamputih']\n",
       "4    positive   ['asiknya', 'nonton', 'hitam', 'putih', 'trans']\n",
       "..        ...                                                ...\n",
       "395  negative  ['banget', 'kesel', 'debat', 'pake', 'emosi', ...\n",
       "396  negative  ['miskin', 'miskin', 'sekolah', 'pungutan', 'l...\n",
       "397  negative  ['emosi', 'cepat', 'tua', 'nonton', 'emosi', '...\n",
       "398  negative  ['penampilan', 'kyk', 'preman', 'taunya', 'bki...\n",
       "399  negative        ['berbelitbelit', 'muter', 'buang', 'mutu']\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import issparse\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('text_preprocessing/final_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_tokens_stemmed</th>\n",
       "      <th>label_sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['undang', 'shanijkt', 'hitamputih', 'pemenang...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['selamat', 'berbuka', 'puasa', 'semoga', 'ama...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['trans', 'hitam', 'putih', 'penghargaan', 'no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['selamat', 'hitamputih']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['asiknya', 'nonton', 'hitam', 'putih', 'trans']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>['banget', 'kesel', 'debat', 'pake', 'emosi', ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>['miskin', 'miskin', 'sekolah', 'pungutan', 'l...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>['emosi', 'cepat', 'tua', 'nonton', 'emosi', '...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>['penampilan', 'kyk', 'preman', 'taunya', 'bki...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>['berbelitbelit', 'muter', 'buang', 'mutu']</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  tweet_tokens_stemmed  label_sentimen\n",
       "0    ['undang', 'shanijkt', 'hitamputih', 'pemenang...               1\n",
       "1    ['selamat', 'berbuka', 'puasa', 'semoga', 'ama...               1\n",
       "2    ['trans', 'hitam', 'putih', 'penghargaan', 'no...               1\n",
       "3                            ['selamat', 'hitamputih']               1\n",
       "4     ['asiknya', 'nonton', 'hitam', 'putih', 'trans']               1\n",
       "..                                                 ...             ...\n",
       "395  ['banget', 'kesel', 'debat', 'pake', 'emosi', ...               0\n",
       "396  ['miskin', 'miskin', 'sekolah', 'pungutan', 'l...               0\n",
       "397  ['emosi', 'cepat', 'tua', 'nonton', 'emosi', '...               0\n",
       "398  ['penampilan', 'kyk', 'preman', 'taunya', 'bki...               0\n",
       "399        ['berbelitbelit', 'muter', 'buang', 'mutu']               0\n",
       "\n",
       "[400 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sentiment labels to binary\n",
    "def convert_label_sentimen(label_sentimen):\n",
    "    return 1 if label_sentimen == \"positive\" else 0\n",
    "\n",
    "df['label_sentimen'] = df['Sentiment'].apply(convert_label_sentimen)\n",
    "df.drop(df.columns[[0]], axis=1, inplace=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= df['tweet_tokens_stemmed'] \n",
    "y= df['label_sentimen']\n",
    "#k-fold cross validation(spliting data)\n",
    "kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self, model_class):\n",
    "        self.model_class = model_class\n",
    "        self.clf = None\n",
    "    \n",
    "    def fit(self, X_train, y_train, selected_features):\n",
    "        self.clf = self.model_class()\n",
    "        if issparse(X_train):\n",
    "            X_train = X_train.toarray()\n",
    "        self.clf.fit(X_train[:, selected_features], y_train)\n",
    "    \n",
    "    def predict(self, X_test, selected_features):\n",
    "        if self.clf is None:\n",
    "            raise ValueError(\"Classifier not fitted. Please call 'fit' method first.\")\n",
    "        if issparse(X_test):\n",
    "            X_test = X_test.toarray()\n",
    "        return self.clf.predict(X_test[:, selected_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(X_train, y_train, selected_features, classifier):\n",
    "    clf = classifier\n",
    "    cv_results = cross_val_score(clf.clf, X_train[:, selected_features], y_train, cv=kfold, scoring='accuracy')\n",
    "    return cv_results.mean()\n",
    "\n",
    "def pso_feature_selection(X_train, y_train, n_particles, inertia, global_weight, local_weight, model_class, tol=1e-5, patience=10):\n",
    "    num_samples, num_features = X_train.shape\n",
    "    bounds = [0, 1]\n",
    "    \n",
    "    num_particles = n_particles\n",
    "    dimensions = num_features\n",
    "    particles = np.random.rand(num_particles, dimensions)\n",
    "    velocities = np.random.rand(num_particles, dimensions) * 0.1\n",
    "    best_positions = particles.copy()\n",
    "    best_scores = np.zeros(num_particles)\n",
    "\n",
    "    global_best_position = np.zeros(dimensions)\n",
    "    global_best_score = 0\n",
    "\n",
    "    no_improvement_count = 0\n",
    "    previous_global_best_score = 0\n",
    "\n",
    "    while no_improvement_count < patience:\n",
    "        for particle in range(num_particles):\n",
    "            r1 = np.random.rand(dimensions)\n",
    "            r2 = np.random.rand(dimensions)\n",
    "            velocities[particle] = (inertia * velocities[particle] +\n",
    "                                    global_weight * r1 * (best_positions[particle] - particles[particle]) +\n",
    "                                    local_weight * r2 * (global_best_position - particles[particle]))\n",
    "\n",
    "            particles[particle] += velocities[particle]\n",
    "            particles[particle] = np.clip(particles[particle], bounds[0], bounds[1])\n",
    "\n",
    "            selected_features = particles[particle] > 0.5\n",
    "            nb = NaiveBayes(model_class)\n",
    "            nb.fit(X_train, y_train, selected_features)\n",
    "            accuracy = evaluate_features(X_train, y_train, selected_features, nb)\n",
    "\n",
    "            if accuracy > best_scores[particle]:\n",
    "                best_scores[particle] = accuracy\n",
    "                best_positions[particle] = particles[particle].copy()\n",
    "\n",
    "            if accuracy > global_best_score:\n",
    "                global_best_score = accuracy\n",
    "                global_best_position = particles[particle].copy()\n",
    "\n",
    "        if abs(global_best_score - previous_global_best_score) < tol:\n",
    "            no_improvement_count += 1\n",
    "        else:\n",
    "            no_improvement_count = 0\n",
    "        \n",
    "        previous_global_best_score = global_best_score\n",
    "\n",
    "    return global_best_position > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Model  Average Accuracy  Average Precision  Average Recall\n",
      "MultinomialNB            0.7950           0.778762           0.830\n",
      "  BernoulliNB            0.7750           0.768636           0.795\n",
      "   GaussianNB            0.7625           0.748291           0.805\n"
     ]
    }
   ],
   "source": [
    "# Define parameters for PSO optimization\n",
    "n_particles = 20\n",
    "inertia = 0.8\n",
    "global_weight = 1\n",
    "local_weight = 0.9\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_class in [MultinomialNB, BernoulliNB, GaussianNB]:\n",
    "    accuracy_scores = []\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "\n",
    "    for train_idx, test_idx in kfold.split(X, y):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        tfidf_model = TfidfVectorizer(smooth_idf=False)\n",
    "        X_train_tfidf = tfidf_model.fit_transform(X_train)\n",
    "        X_test_tfidf = tfidf_model.transform(X_test)\n",
    "\n",
    "        X_train_dense = X_train_tfidf.toarray()\n",
    "        selected_features = pso_feature_selection(X_train_dense, y_train, n_particles, inertia, global_weight, local_weight, model_class)\n",
    "\n",
    "        if not np.any(selected_features):\n",
    "            selected_features[0] = True\n",
    "\n",
    "        nb = NaiveBayes(MultinomialNB)\n",
    "        nb.fit(X_train_tfidf, y_train, selected_features)\n",
    "        y_pred = nb.predict(X_test_tfidf.toarray(), selected_features)\n",
    "\n",
    "        accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
    "        precision_scores.append(precision_score(y_test, y_pred))\n",
    "        recall_scores.append(recall_score(y_test, y_pred))\n",
    "\n",
    "    fold_results = {\n",
    "        \"Model\": model_class.__name__,\n",
    "        \"Average Accuracy\": np.mean(accuracy_scores),\n",
    "        \"Average Precision\": np.mean(precision_scores),\n",
    "        \"Average Recall\": np.mean(recall_scores)\n",
    "    }\n",
    "\n",
    "    results.append(fold_results)\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
